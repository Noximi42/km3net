# ConvNet applied on CIFAR-10 dataset, with no data augmentation.

command = Train #:Eval

precision = "float"; traceLevel = 1 ; deviceId = "auto"
profilerEnabled = true
parallelTrain = true

rootDir = "/home/woody/capn/mppi027h/km3net" ; dataDir = "$rootDir$/Data/numuEReg" ;
outputDir = "$rootDir$/OutputReg" ;

modelPath = "$outputDir$/Models/ConvNetEnergyReg"


Train = {
    action = "train"

    BrainScriptNetworkBuilder = {
        imageShape = 11:11:18:50:1
        labelDim = 2

        featMean = 128
        featScale = 1/256
        Normalize{m,f} = x => f .* (x - m)

        model = Sequential (
            #Normalize {featMean, featScale} :
            ConvolutionalLayer {4, (3:3:3:3), pad = true} : ReLU :  
			  MaxPoolingLayer {(2:2:2:2), stride = (2:2:2:2), pad = true} :	#(6:6:9:25)
			ConvolutionalLayer {32, (3:3:3:3), pad = true} : ReLU : 
			ConvolutionalLayer {32, (3:3:3:3), pad = true} : ReLU : 
			  MaxPoolingLayer {(1:1:1:2), stride = (1:1:1:2)} :  			#(6:6:9:12)
			ConvolutionalLayer {32, (3:3:3:3), pad = true} : ReLU : 
			ConvolutionalLayer {32, (3:3:3:3), pad = true} : ReLU : 
			  MaxPoolingLayer {(2:2:3:2), stride = (2:2:3:2)} : 			#(3:3:3:6)
			ConvolutionalLayer {128, (3:3:3:3), pad = true} : ReLU : 
			ConvolutionalLayer {128, (3:3:3:3), pad = true} : ReLU : 
			  MaxPoolingLayer {(1:1:1:2), stride = (1:1:1:2)} :				#(3:3:3:3)
			
			  
			            
            DenseLayer {256} : ReLU : Dropout : 
            DenseLayer {128} : ReLU : Dropout : 
            LinearLayer {labelDim}
        )

        # inputs
        features = Input {imageShape}
        labels   = Input {labelDim}

        # apply model to features
        z = model (features)

        # define regression loss
        diff = labels - z
        sqerr = ReduceSum (diff.*diff, axis=1)
        rmse =  Sqrt (sqerr / labelDim)

        featureNodes    = (features)
        labelNodes      = (labels)
        criterionNodes  = (rmse)
        evaluationNodes = (rmse)
        outputNodes     = (z)
    }

    SGD = {
        epochSize = 50000 
        minibatchSize = 64

        learningRatesPerSample = 0.00015625*1:0.000046875*1:0.000015625*1:0.0000046875
        momentumAsTimeConstant = 600*5:6400
        maxEpochs = 100
        L2RegWeight = 0.03
        dropoutRate = 0*10:0.5

        #firstMBsToShowResult = 10 ; numMBsToShowResult = 100
		
    }

    reader = {
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/TestData.txt"
        randomize = true
        keepDataInMemory = false     # cache all data in memory 	 
        input = {
            features = { dim = 108900 ; format = "dense" }
            labels   = { dim = 2 ;   format = "dense" }
        }
    }
}

# Eval action
Eval = {
    action = "eval"
    minibatchSize = 16
    evalNodeNames = errs:top5Errs  # also test top-5 error rate
    reader = {
        readerType = "CNTKTextFormatReader"
        file = "$DataDir$/TestData.txt"
        input = {
            features = { dim = 108900 ; format = "dense" }
            labels   = { dim = 2 ;   format = "dense" }
        }
    }
}
